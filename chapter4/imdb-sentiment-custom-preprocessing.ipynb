{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9ea237-bdb2-4842-bce2-7a61574a3fe1",
   "metadata": {},
   "source": [
    "This notebook is an attempt to do custom data preprocessing of the IMDB dataset. Keras provides a nice, tokenized version of the data as in `imdb-sentiment.ipynb` but it's useful to learn to do this stuff on our own. Following preprocessing recipe from: https://www.kaggle.com/code/affand20/imdb-with-pytorch/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7e88c96-4245-4bac-a4e7-a27b27758bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69c34d1-5faa-4f54-a59a-1d6befa83d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download data into data/ from: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "data = pd.read_csv(\"data/IMDB_Dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fecf37b-3bb2-4903-baa3-50bbab2521c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add binary column\n",
    "bin_label = [1 if sent == 'positive' else 0 for sent in data['sentiment'].tolist()]\n",
    "data['label'] = bin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4576ba-6a6e-49d9-804d-ee0c397aec01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50000.000000\n",
       "mean       231.156940\n",
       "std        171.343997\n",
       "min          4.000000\n",
       "25%        126.000000\n",
       "50%        173.000000\n",
       "75%        280.000000\n",
       "max       2470.000000\n",
       "Name: review, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek into text lengths to figure out truncation\n",
    "lengths = pd.Series(data['review'].apply(lambda t: len(t.split())))\n",
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d782ddbe-b822-413b-8894-89d645790ba8",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59392b11-c08a-44b2-8f46-dae17895d78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/muhammadali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/muhammadali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97c25c-ab73-4480-bb1d-8751b4613d75",
   "metadata": {},
   "source": [
    "Example of how the same would be done in spaCy -- but choosing nltk because it's the simpler pipeline\n",
    "\n",
    "```\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(data.review.iloc[0])\n",
    "\n",
    "def spacy_tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [tok.lemma_ for tok in doc]\n",
    "\n",
    "tokens = data.review.apply(spacy_tokenize)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa2ab7e-8ca6-4a63-a143-26e0647e1c07",
   "metadata": {},
   "source": [
    "#### Text cleaning pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97a5e535-73a7-4c3d-9599-2253e55a396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_link(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def rm_inner_punct(text):\n",
    "    # remove punctuation inside sentences\n",
    "    return re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', '', text)\n",
    "\n",
    "def rm_html(text):\n",
    "    return re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "def space_bw_punct(text):\n",
    "    pattern = r'([.,!?-])'\n",
    "    s = re.sub(pattern, r' \\1 ', text)     # add whitespaces between punctuation\n",
    "    s = re.sub(r'\\s{2,}', ' ', s)        # remove double whitespaces    \n",
    "    return s\n",
    "\n",
    "def rm_number(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def rm_whitespaces(text):\n",
    "    return re.sub(r' +', ' ', text)\n",
    "\n",
    "def rm_nonascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "def space_bt_punct(text):\n",
    "    pattern = r'([.,!?-])'\n",
    "    s = re.sub(pattern, r' \\1 ', text)     # add whitespaces between punctuation\n",
    "    s = re.sub(r'\\s{2,}', ' ', s)        # remove double whitespaces    \n",
    "    return s\n",
    "\n",
    "def rm_punct2(text):\n",
    "    # return re.sub(r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "    return re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "\n",
    "def spell_correction(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "\n",
    "def rm_emoji(text):\n",
    "    emojis = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emojis.sub(r'', text)\n",
    "\n",
    "def clean_pipeline(text):    \n",
    "    # concatenate all cleaning operations together\n",
    "    no_link = rm_link(text)\n",
    "    no_html = rm_html(no_link)\n",
    "    space_punct = space_bt_punct(no_html)\n",
    "    no_punct = rm_punct2(space_punct)\n",
    "    no_number = rm_number(no_punct)\n",
    "    no_whitespaces = rm_whitespaces(no_number)\n",
    "    no_nonasci = rm_nonascii(no_whitespaces)\n",
    "    no_emoji = rm_emoji(no_nonasci)\n",
    "    spell_corrected = spell_correction(no_emoji)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4397418-6375-4835-8807-da407cbeaefe",
   "metadata": {},
   "source": [
    "#### Tokenizing and lemmatizing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430890b9-6106-4330-abd3-7944057feaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def rm_stopwords(tokens):\n",
    "    return [tok for tok in tokens if tok not in stopwords]\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(tok) for tok in text]\n",
    "    return rm_stopwords(lemmas)\n",
    "\n",
    "def preprocess_pipeline(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = rm_stopwords(tokens)\n",
    "    lemmas = lemmatize(tokens)\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1d5f31-adfb-4dcc-8e84-b7255e1b6b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e301a75dfa64450d99d50f9b54f70379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4790754d53f499da0d93e4602e6dabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['clean'] = data.review.progress_apply(clean_pipeline)\n",
    "data['processed'] = data.review.progress_apply(preprocess_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62ab5111-f497-4026-9bad-effe6e6b3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export processed data\n",
    "data[['clean', 'processed']].to_csv('imdb-processed.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87340a-ed55-4195-b2e6-38b8fd7907c4",
   "metadata": {},
   "source": [
    "### Vectorize data into model-compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cb7e850-b5c8-4ad2-8e1e-6204f39584df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'reviewer', 'mentioned', 'watching', '1', 'Oz', 'episode', \"'ll\", 'hooked', '.']\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary from all texts\n",
    "reviews = data.processed.values\n",
    "all_reviews = ' '.join(reviews)\n",
    "words = all_reviews.split()\n",
    "\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c8e9d771-b409-4582-a4f4-34c8ce329809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "NUM_WORDS = 20000\n",
    "counter = Counter(words)\n",
    "# sort words in descending order of frequency (stop words have been removed)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "id2word = dict(enumerate(vocab[:NUM_WORDS], 1))\n",
    "id2word[0] = '<PAD>'\n",
    "word2id = {word: id for id, word in id2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b8d96188-a117-4271-b96c-04df345afbf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50d6fda7c844f53aa9d417829214130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vectorize sentences based on vocab\n",
    "reviews_enc = []\n",
    "for review in tqdm(reviews):\n",
    "    # NOTE: this is a whitespace split, the nltk pipeline should not have joined the texts back\n",
    "    curr_review = []\n",
    "    for word in review.split():\n",
    "        try:\n",
    "            curr_review.append(word2id[word])\n",
    "        except KeyError:\n",
    "            # i'm only recording the most frequent 20,000 words\n",
    "            pass\n",
    "\n",
    "    reviews_enc.append(curr_review)\n",
    "    # reviews_enc.append([word2id[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd40ebc-9687-4b90-ba17-d604819ad25e",
   "metadata": {},
   "source": [
    "This `reviews_enc` is the form that keras exports its dataset in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1ecab5b7-3395-4333-b9bb-0489ccfcbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM=NUM_WORDS+1\n",
    "\n",
    "def vectorize_sequences(sequences, dim=DIM):\n",
    "    matrix = np.zeros((len(sequences), dim))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for word_id in sequence:\n",
    "            matrix[i, word_id] = 1.0\n",
    "\n",
    "    return matrix\n",
    "\n",
    "reviews_vec = vectorize_sequences(reviews_enc, len(vocab)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c85341f4-8b1b-472a-9211-d81245b87e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of the whole vocab, we can vectorize to a fixed seq_len with padding tokens\n",
    "def vectorize_pad(reviews, pad_id, seq_len=128):\n",
    "    # reviews_vec is a [num_reviews, seq_len] shape matrix with <PAD> tokens throughout\n",
    "    reviews_vec = np.full((len(reviews), seq_len), pad_id, dtype=int)\n",
    "    # fill the first seq_len tokens with the actual tokens instead of <PAD>\n",
    "    for i, sequence in enumerate(reviews):\n",
    "        # what happens if len(sequence) > seq_len?\n",
    "        reviews_vec[i, :len(sequence)] = np.array(sequence)[:seq_len]\n",
    "\n",
    "    return reviews_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0b2ac642-5858-40da-92e0-0a8b50e0bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle before vectorizing\n",
    "# X = vectorize_pad(reviews_enc, word2id['<PAD>'], seq_len=256)\n",
    "X = vectorize_sequences(reviews_enc)\n",
    "y = np.array(data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913ed57-9ac3-4e58-9b7d-28c72a4d9719",
   "metadata": {},
   "source": [
    "**Train-val-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9c97a303-ce1e-446a-b870-ea8c8d37d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=.8\n",
    "val_size=.05\n",
    "\n",
    "train_i = int(train_size * X.shape[0])\n",
    "X_train = X[:train_i]\n",
    "y_train = y[:train_i]\n",
    "\n",
    "val_i = int(val_size * X.shape[0])\n",
    "X_val = X[train_i:train_i+val_i]\n",
    "y_val = y[train_i:train_i+val_i]\n",
    "\n",
    "X_test = X[train_i+val_i:]\n",
    "y_test = y[train_i+val_i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7c21d524-0687-4592-86af-16f699f17930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aeff54fb-f4b7-41a5-a48d-d766ba9cf45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = keras.Sequential([\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "MLP.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1d2532d0-83d6-4591-90d8-5997674cbd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "79/79 [==============================] - 1s 13ms/step - loss: 0.3970 - accuracy: 0.8557 - val_loss: 0.2850 - val_accuracy: 0.8964\n",
      "Epoch 2/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.2115 - accuracy: 0.9249 - val_loss: 0.2731 - val_accuracy: 0.8920\n",
      "Epoch 3/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.1589 - accuracy: 0.9441 - val_loss: 0.2785 - val_accuracy: 0.8956\n",
      "Epoch 4/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.1267 - accuracy: 0.9561 - val_loss: 0.2953 - val_accuracy: 0.8936\n",
      "Epoch 5/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.1014 - accuracy: 0.9658 - val_loss: 0.3189 - val_accuracy: 0.8920\n",
      "Epoch 6/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0804 - accuracy: 0.9733 - val_loss: 0.3563 - val_accuracy: 0.8876\n",
      "Epoch 7/20\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.0639 - accuracy: 0.9802 - val_loss: 0.3830 - val_accuracy: 0.8868\n",
      "Epoch 8/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0499 - accuracy: 0.9843 - val_loss: 0.4223 - val_accuracy: 0.8836\n",
      "Epoch 9/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0373 - accuracy: 0.9896 - val_loss: 0.4555 - val_accuracy: 0.8828\n",
      "Epoch 10/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0272 - accuracy: 0.9922 - val_loss: 0.5159 - val_accuracy: 0.8784\n",
      "Epoch 11/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0201 - accuracy: 0.9947 - val_loss: 0.5570 - val_accuracy: 0.8776\n",
      "Epoch 12/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0135 - accuracy: 0.9969 - val_loss: 0.6204 - val_accuracy: 0.8756\n",
      "Epoch 13/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0103 - accuracy: 0.9978 - val_loss: 0.6972 - val_accuracy: 0.8752\n",
      "Epoch 14/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.8243 - val_accuracy: 0.8708\n",
      "Epoch 15/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.7820 - val_accuracy: 0.8752\n",
      "Epoch 16/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 0.8977 - val_accuracy: 0.8740\n",
      "Epoch 17/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.9559 - val_accuracy: 0.8704\n",
      "Epoch 18/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 1.0054 - val_accuracy: 0.8724\n",
      "Epoch 19/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 6.2143e-04 - accuracy: 0.9998 - val_loss: 1.1013 - val_accuracy: 0.8704\n",
      "Epoch 20/20\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 3.5245e-04 - accuracy: 0.9999 - val_loss: 1.1925 - val_accuracy: 0.8696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x384c50d90>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c6502-bc1d-4d32-9e3c-dc16bab7ccf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-book",
   "language": "python",
   "name": "dl-book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
